% VideoSHAP References

@article{google2024gemini,
  title={Gemini 2.0: A Family of Highly Capable Multimodal Models},
  author={{Google DeepMind}},
  journal={Technical Report},
  year={2024},
  publisher={Google}
}

@article{openai2024gpt4o,
  title={{GPT-4o} System Card},
  author={{OpenAI}},
  journal={Technical Report},
  year={2024},
  publisher={OpenAI}
}

@article{bai2023qwen,
  title={Qwen-{VL}: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@article{goldshmidt2025pixelshap,
  title={Pixel{SHAP}: Attention Please! What Vision-Language Models Actually Focus On},
  author={Goldshmidt, Roni},
  journal={arXiv preprint arXiv:2503.06670},
  year={2025}
}

@article{horovicz2025agentshap,
  title={Agent{SHAP}: Interpreting {LLM} Agent Tool Importance with Monte Carlo Shapley Value Estimation},
  author={Horovicz, Miriam and Goldshmidt, Roni},
  journal={arXiv preprint arXiv:2512.12597},
  year={2025}
}

@inproceedings{horovicz2024tokenshap,
  title={Token{SHAP}: Interpreting Large Language Models with {M}onte {C}arlo {S}hapley Value Estimation},
  author={Horovicz, Miriam and Goldshmidt, Roni},
  booktitle={EMNLP Workshop on NLP for Science},
  year={2024}
}

@inproceedings{selvaraju2017gradcam,
  title={Grad-{CAM}: Visual Explanations from Deep Networks via Gradient-based Localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  pages={618--626},
  year={2017}
}

@inproceedings{petsiuk2018rise,
  title={{RISE}: Randomized Input Sampling for Explanation of Black-box Models},
  author={Petsiuk, Vitali and Das, Abir and Saenko, Kate},
  booktitle={British Machine Vision Conference (BMVC)},
  year={2018}
}

@inproceedings{petsiuk2021drise,
  title={{D-RISE}: Dynamic Randomized Input Sampling for Object Detection Explanations},
  author={Petsiuk, Vitali and Das, Abir and Saenko, Kate},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021}
}

@article{parcalabescu2022mmshap,
  title={{MM-SHAP}: Multimodal {S}hapley Values for Model Interpretation},
  author={Parcalabescu, Letitia and Frank, Anette},
  journal={arXiv preprint arXiv:2212.03582},
  year={2022}
}

@article{ravi2024sam2,
  title={{SAM} 2: Segment Anything in Images and Videos},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolber, Chloe and Gustafson, Laura and others},
  journal={arXiv preprint arXiv:2408.00714},
  year={2024}
}

@article{meta2025sam3,
  title={{SAM} 3: Unified Segmentation and Tracking},
  author={{Meta AI}},
  journal={Technical Report},
  year={2025},
  publisher={Meta}
}

@inproceedings{zhang2023videollama,
  title={Video-{LLaMA}: An Instruction-tuned Audio-Visual Language Model for Video Understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023}
}

@article{wang2021temporal,
  title={Temporal Saliency Detection for Video Understanding},
  author={Wang, Jianqiang and Jiao, Yuhan and Li, Zhuang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021}
}

@inproceedings{arnab2021vivit,
  title={{ViViT}: A Video Vision Transformer},
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lucic, Mario and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages={6836--6846},
  year={2021}
}

@inproceedings{kirillov2023sam,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2023}
}

@article{liu2023grounding,
  title={Grounding {DINO}: Marrying {DINO} with Grounded Pre-Training for Open-Set Object Detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},
  journal={arXiv preprint arXiv:2303.05499},
  year={2023}
}

@article{shapley1953value,
  title={A Value for n-Person Games},
  author={Shapley, Lloyd S},
  journal={Contributions to the Theory of Games},
  volume={2},
  number={28},
  pages={307--317},
  year={1953},
  publisher={Princeton University Press}
}

@inproceedings{lundberg2017shap,
  title={A Unified Approach to Interpreting Model Predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={30},
  pages={4765--4774},
  year={2017}
}

@inproceedings{ribeiro2016lime,
  title={``{W}hy Should {I} Trust You?'': Explaining the Predictions of Any Classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={1135--1144},
  year={2016}
}

@inproceedings{abnar2020attention,
  title={Quantifying Attention Flow in Transformers},
  author={Abnar, Samira and Zuidema, Willem},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages={4190--4197},
  year={2020}
}

@inproceedings{fong2017meaningful,
  title={Interpretable Explanations of Black Boxes by Meaningful Perturbation},
  author={Fong, Ruth C and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  pages={3429--3437},
  year={2017}
}

@inproceedings{li2023videochat,
  title={Video{C}hat: Chat-Centric Video Understanding},
  author={Li, Kunchang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  booktitle={arXiv preprint arXiv:2305.06355},
  year={2023}
}

@inproceedings{liu2024llava,
  title={Visual Instruction Tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2024}
}

@article{wang2024internvideo2,
  title={Intern{V}ideo2: Scaling Video Foundation Models for Multimodal Video Understanding},
  author={Wang, Yi and Li, Kunchang and Li, Xinhao and Yu, Jiashuo and He, Yinan and Chen, Guo and Pei, Baoqi and Zheng, Rongkun and Xu, Jilan and Wang, Zun and others},
  journal={arXiv preprint arXiv:2403.15377},
  year={2024}
}

@inproceedings{mangalam2024egoschema,
  title={Ego{S}chema: A Diagnostic Benchmark for Very Long-form Video Language Understanding},
  author={Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2024}
}

@inproceedings{soomro2012ucf101,
  title={{UCF101}: A Dataset of 101 Human Actions Classes From Videos in The Wild},
  author={Soomro, Khurram and Zamir, Amir Roshan and Shah, Mubarak},
  booktitle={arXiv preprint arXiv:1212.0402},
  year={2012}
}

@inproceedings{tsiami2020saliency,
  title={Video Saliency Detection Using Spatiotemporal Deep Learning Networks},
  author={Tsiami, Antigoni and Koutras, Petros and Maragos, Petros},
  booktitle={European Conference on Computer Vision (ECCV)},
  year={2020}
}

@inproceedings{stergiou2019temporal,
  title={Temporal {G}rad-{CAM} for Video Classification},
  author={Stergiou, Alexandros and Poppe, Ronald and Veltkamp, Remco C},
  booktitle={arXiv preprint arXiv:1909.07321},
  year={2019}
}

@inproceedings{bertasius2021timesformer,
  title={Is Space-Time Attention All You Need for Video Understanding?},
  author={Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  booktitle={Proceedings of the International Conference on Machine Learning (ICML)},
  pages={813--824},
  year={2021}
}

@inproceedings{yu2020bdd100k,
  title={{BDD100K}: A Diverse Driving Dataset for Heterogeneous Multitask Learning},
  author={Yu, Fisher and Chen, Haofeng and Wang, Xin and Xian, Wenqi and Chen, Yingying and Liu, Fangchen and Madhavan, Vashisht and Darrell, Trevor},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={2636--2645},
  year={2020}
}

@inproceedings{caesar2020nuscenes,
  title={nu{S}cenes: A Multimodal Dataset for Autonomous Driving},
  author={Caesar, Holger and Bankiti, Varun and Lang, Alex H and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={11621--11631},
  year={2020}
}
